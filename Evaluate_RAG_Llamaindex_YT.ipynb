{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Reference\n",
        "https://cookbook.openai.com/examples/evaluation/evaluate_rag_with_llamaindex"
      ],
      "metadata": {
        "id": "SQF-2bB7FzbB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUZgo8TSBvjn",
        "outputId": "7347d8b9-0b37-4fe4-8d72-7ce693b13061"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-index in /usr/local/lib/python3.10/dist-packages (0.10.19)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.2.0,>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.5)\n",
            "Requirement already satisfied: llama-index-cli<0.2.0,>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.9)\n",
            "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.19 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.10.19)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.2.0,>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.6)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.4)\n",
            "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.9.48)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.9)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.4)\n",
            "Requirement already satisfied: llama-index-program-openai<0.2.0,>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.4)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.2.0,>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.3)\n",
            "Requirement already satisfied: llama-index-readers-file<0.2.0,>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.11)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse<0.2.0,>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.3)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.19->llama-index) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.19->llama-index) (2.0.28)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.19->llama-index) (3.9.3)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.19->llama-index) (0.6.4)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.19->llama-index) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.19->llama-index) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.19->llama-index) (2023.6.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.19->llama-index) (0.27.0)\n",
            "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.13 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.19->llama-index) (0.1.13)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.19->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.19->llama-index) (3.2.1)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.19->llama-index) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.19->llama-index) (1.25.2)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.19->llama-index) (1.14.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.19->llama-index) (1.5.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.19->llama-index) (9.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.19->llama-index) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.19->llama-index) (8.2.3)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.19->llama-index) (0.6.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.19->llama-index) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.19->llama-index) (4.10.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.19->llama-index) (0.9.0)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (4.12.3)\n",
            "Requirement already satisfied: bs4<0.0.3,>=0.0.2 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (0.0.2)\n",
            "Requirement already satisfied: pymupdf<2.0.0,>=1.23.21 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (1.23.26)\n",
            "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (4.1.0)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (0.0.26)\n",
            "Requirement already satisfied: llama-parse<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index) (0.3.9)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.19->llama-index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.19->llama-index) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.19->llama-index) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.19->llama-index) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.19->llama-index) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.19->llama-index) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (2.5)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.9.3->llama-index-core<0.11.0,>=0.10.19->llama-index) (1.14.1)\n",
            "Requirement already satisfied: pydantic>=1.10 in /usr/local/lib/python3.10/dist-packages (from llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.19->llama-index) (2.6.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.19->llama-index) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.19->llama-index) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.19->llama-index) (1.0.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.19->llama-index) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.19->llama-index) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.19->llama-index) (0.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.19->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.19->llama-index) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.19->llama-index) (2023.12.25)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.19->llama-index) (1.7.0)\n",
            "Requirement already satisfied: PyMuPDFb==1.23.22 in /usr/local/lib/python3.10/dist-packages (from pymupdf<2.0.0,>=1.23.21->llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (1.23.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.19->llama-index) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.19->llama-index) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.19->llama-index) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.19->llama-index) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.19->llama-index) (3.21.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.19->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.19->llama-index) (2023.4)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.19->llama-index) (1.2.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.19->llama-index) (24.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.19->llama-index) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.19->llama-index) (2.16.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->llama-index-core<0.11.0,>=0.10.19->llama-index) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The nest_asyncio module enables the nesting of asynchronous functions within an already running async loop.\n",
        "# This is necessary because Jupyter notebooks inherently operate in an asynchronous loop.\n",
        "# By applying nest_asyncio, we can run additional async functions within this existing loop without conflicts.\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from llama_index.core.evaluation import generate_question_context_pairs\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
        "from llama_index.core.node_parser import SimpleNodeParser\n",
        "from llama_index.core.evaluation import generate_question_context_pairs\n",
        "from llama_index.core.evaluation import RetrieverEvaluator\n",
        "from llama_index.llms import openai\n",
        "\n",
        "import os\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "7fVNCCgeE5Rn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "U7Aw251nHd32"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zCyxQhHIMS4",
        "outputId": "b3b8032f-c8c7-45dc-c418-10c330954f8a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Data and Build Index**"
      ],
      "metadata": {
        "id": "J6SFBmriOY5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = SimpleDirectoryReader(\"/content/drive/MyDrive/PDF\").load_data()\n",
        "\n",
        "# Define an LLM\n",
        "# llm = gemma_lm\n",
        "\n",
        "# Build index with a chunk_size of 512\n",
        "node_parser = SimpleNodeParser.from_defaults(chunk_size=512)\n",
        "nodes = node_parser.get_nodes_from_documents(documents)\n",
        "vector_index = VectorStoreIndex(nodes)"
      ],
      "metadata": {
        "id": "rtCHjlzrHr3z"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build a QueryEngine and start querying.**"
      ],
      "metadata": {
        "id": "o7bajEHcOa8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = vector_index.as_query_engine()"
      ],
      "metadata": {
        "id": "FU_5HIDlIG41"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check response**"
      ],
      "metadata": {
        "id": "oMdGeOTpfid7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response_vector = query_engine.query(\"What is difference between computer vision and Deep Learning?\")"
      ],
      "metadata": {
        "id": "Ji_8I-LYeHNa"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_vector.response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "gP8ZDDfeePge",
        "outputId": "edc86ba0-f1b9-430f-bd24-a1489a29251d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Computer vision is the computer's ability to extract information and insights from images and videos, while deep learning is a method in artificial intelligence that teaches computers to process data in a way inspired by the human brain.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response_vector = query_engine.query(\"Who is captain of indian football team?\")"
      ],
      "metadata": {
        "id": "HKREQ0qMfcG8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_vector.response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "8IF4-NyHOw1K",
        "outputId": "55a31299-7d81-4fea-814d-741c275c21cb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I'm unable to provide an answer to that question as it is not related to the context provided.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**By default it retrieves two similar nodes**"
      ],
      "metadata": {
        "id": "NeeFaHBgRU-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First retrieved node\n",
        "response_vector.source_nodes[0].get_text()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "txXdtOLKO0A-",
        "outputId": "75b8f625-6f25-4e1b-be4f-384f202642ed"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Virtual\\nassistants\\nsuch\\nas\\nAmazon\\nAlexa\\nand\\nautomatic\\ntranscription\\nsoftware\\nuse\\nspeech\\nrecognition\\nto\\ndo\\nthe\\nfollowing\\ntasks:\\n●\\nAssist\\ncall\\ncenter\\nagents\\nand\\nautomatically\\nclassify\\ncalls.\\n●\\nConvert\\nclinical\\nconversations\\ninto\\ndocumentation\\nin\\nreal\\ntime.\\n●\\nAccurately\\nsubtitle\\nvideos\\nand\\nmeeting\\nrecordings\\nfor\\na\\nwider\\ncontent\\nreach.\\nNatural\\nlanguage\\nprocessing\\nComputers\\nuse\\ndeep\\nlearning\\nalgorithms\\nto\\ngather\\ninsights\\nand\\nmeaning\\nfrom\\ntext\\ndata\\nand\\ndocuments\\n.\\nThis\\nability\\nto\\nprocess\\nnatural,\\nhuman-created\\ntext\\nhas\\nseveral\\nuse\\ncases,\\nincluding\\nin\\nthese\\nfunctions:\\n●\\nAutomated\\nvirtual\\nagents\\nand\\nchatbots\\n●\\nAutomatic\\nsummarization\\nof\\ndocuments\\nor\\nnews\\narticles\\n●\\nBusiness\\nintelligence\\nanalysis\\nof\\nlong-form\\ndocuments,\\nsuch\\nas\\nemails\\nand\\nforms\\n●\\nIndexing\\nof\\nkey\\nphrases\\nthat\\nindicate\\nsentiment,\\nsuch\\nas\\npositive\\nand\\nnegative\\ncomments\\non\\nsocial\\nmedia\\nRecommendation\\nengines\\nApplications\\ncan\\nuse\\ndeep\\nlearning\\nmethods\\nto\\ntrack\\nuser\\nactivity\\nand\\ndevelop\\npersonalized\\nrecommendations\\n.\\nThey\\ncan\\nanalyze\\nthe\\nbehavior\\nof\\nvarious\\nusers\\nand\\nhelp\\nthem\\ndiscover\\nnew\\nproducts\\nor\\nservices.\\nFor\\nexample,\\nmany\\nmedia\\nand\\nentertainment\\ncompanies,\\nsuch\\nas\\nNetflix,\\nFox,\\nand\\nPeacock,\\nuse\\ndeep\\nlearning\\nto\\ngive\\npersonalized\\nvideo\\nrecommendations.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Second retrieved node\n",
        "response_vector.source_nodes[1].get_text()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "xZ6arCNsRfMd",
        "outputId": "bcd1806f-0ca5-488a-c828-5af1bc8b24b5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"You\\ncan\\ngroup\\nthese\\nvarious\\nuse\\ncases\\nof\\ndeep\\nlearning\\ninto\\nfour\\nbroad\\ncategories—computer\\nvision,\\nspeech\\nrecognition,\\nnatural\\nlanguage\\nprocessing\\n(NLP),\\nand\\nrecommendation\\nengines.\\nComputer\\nvision\\nComputer\\nvision\\nis\\nthe\\ncomputer's\\nability\\nto\\nextract\\ninformation\\nand\\ninsights\\nfrom\\nimages\\nand\\nvideos.\\nComputers\\ncan\\nuse\\ndeep\\nlearning\\ntechniques\\nto\\ncomprehend\\nimages\\nin\\nthe\\nsame\\nway\\nthat\\nhumans\\ndo.\\nComputer\\nvision\\nhas\\nseveral\\napplications,\\nsuch\\nas\\nthe\\nfollowing:\\n●\\nContent\\nmoderation\\nto\\nautomatically\\nremove\\nunsafe\\nor\\ninappropriate\\ncontent\\nfrom\\nimage\\nand\\nvideo\\narchives\\n●\\nFacial\\nrecognition\\nto\\nidentify\\nfaces\\nand\\nrecognize\\nattributes\\nlike\\nopen\\neyes,\\nglasses,\\nand\\nfacial\\nhair\\n●\\nImage\\nclassification\\nto\\nidentify\\nbrand\\nlogos,\\nclothing,\\nsafety\\ngear ,\\nand\\nother\\nimage\\ndetails\\nSpeech\\nrecognition\\nDeep\\nlearning\\nmodels\\ncan\\nanalyze\\nhuman\\nspeech\\ndespite\\nvarying\\nspeech\\npatterns,\\npitch,\\ntone,\\nlanguage,\\nand\\naccent.\\nVirtual\\nassistants\\nsuch\\nas\\nAmazon\\nAlexa\\nand\\nautomatic\\ntranscription\\nsoftware\\nuse\\nspeech\\nrecognition\\nto\\ndo\\nthe\\nfollowing\\ntasks:\\n●\\nAssist\\ncall\\ncenter\\nagents\\nand\\nautomatically\\nclassify\\ncalls.\\n●\\nConvert\\nclinical\\nconversations\\ninto\\ndocumentation\\nin\\nreal\\ntime.\\n●\\nAccurately\\nsubtitle\\nvideos\\nand\\nmeeting\\nrecordings\\nfor\\na\\nwider\\ncontent\\nreach.\\nNatural\\nlanguage\\nprocessing\\nComputers\\nuse\\ndeep\\nlearning\\nalgorithms\\nto\\ngather\\ninsights\\nand\\nmeaning\\nfrom\\ntext\\ndata\\nand\\ndocuments\\n.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response_vector.source_nodes[2].get_text()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "9ZUYw-njRhdH",
        "outputId": "6909f801-9bb1-477f-9f4b-5c075e0bb6e4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-6207fc4675af>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresponse_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource_nodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "But you can modify using\n",
        "**vector_index.as_query_engine(similarity_top_k=k).**"
      ],
      "metadata": {
        "id": "TiV6BlYnRo1Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Evaluation"
      ],
      "metadata": {
        "id": "-HkYyOqTRwoQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a RAG system, evaluation focuses on two critical aspects:\n",
        "\n",
        "**Retrieval Evaluation:** This assesses the accuracy and relevance of the information retrieved by the system.\n",
        "\n",
        "**Response Evaluation:** This measures the quality and appropriateness of the responses generated by the system based on the retrieved information."
      ],
      "metadata": {
        "id": "EojVoFIfR1nL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LlamaIndex offers a generate_question_context_pairs module specifically for crafting questions and context pairs which can be used in the assessment of the RAG system of both Retrieval and Response Evaluation."
      ],
      "metadata": {
        "id": "xWPiQcj4Upgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "qa_dataset = generate_question_context_pairs(\n",
        "    nodes,\n",
        "    llm=OpenAI(),\n",
        "    num_questions_per_chunk=2\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yh0-PuY9Rk1f",
        "outputId": "f1e20b8a-e837-4b83-a333-6d8bb987aabd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:26<00:00,  6.60s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(qa_dataset.queries.values())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLGi89XQSEsP",
        "outputId": "5a4a6255-407c-45d9-a4be-5d1863c63628"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['How does deep learning differ from traditional methods of processing data, and how is it inspired by the human brain?',\n",
              " 'Provide examples of everyday products and emerging technologies that utilize deep learning technology, and explain how it enhances their functionality.',\n",
              " 'How do businesses utilize deep learning models in various applications, and what are some examples of industries where deep learning is commonly used?',\n",
              " 'Provide examples of specific applications of deep learning in self-driving cars, defense systems, medical image analysis, and factory operations, highlighting the importance of automation and detection capabilities in each scenario.',\n",
              " 'How can deep learning techniques be applied in computer vision, and what are some specific applications of computer vision mentioned in the text?',\n",
              " 'Explain the role of speech recognition in utilizing deep learning models, and provide examples of tasks that virtual assistants and automatic transcription software can perform using speech recognition technology.',\n",
              " 'How do virtual assistants like Amazon Alexa and automatic transcription software utilize speech recognition technology in various tasks, and what are some specific examples provided in the text?',\n",
              " 'Explain the role of deep learning algorithms in natural language processing and provide examples of how this technology is used in automated virtual agents, document summarization, and sentiment analysis.']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Retrieval Evaluation:**"
      ],
      "metadata": {
        "id": "MmHjNjdMUvTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vector_index.as_retriever(similarity_top_k=2)"
      ],
      "metadata": {
        "id": "dDNLGrTKU4Xf"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hit Rate:**\n",
        "\n",
        "Hit rate calculates the fraction of queries where the correct answer is found within the top-k retrieved documents. In simpler terms, it’s about how often our system gets it right within the top few guesses.\n",
        "\n",
        "**Mean Reciprocal Rank (MRR):**\n",
        "\n",
        "For each query, MRR evaluates the system’s accuracy by looking at the rank of the highest-placed relevant document. Specifically, it’s the average of the reciprocals of these ranks across all the queries. So, if the first relevant document is the top result, the reciprocal rank is 1; if it’s second, the reciprocal rank is 1/2, and so on."
      ],
      "metadata": {
        "id": "L9Q8Ll4fVF9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
        "    [\"mrr\", \"hit_rate\"], retriever=retriever\n",
        ")"
      ],
      "metadata": {
        "id": "S4FnMv3nVIE3"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate\n",
        "eval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset)"
      ],
      "metadata": {
        "id": "HXW2-mg6V-jh"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_results(name, eval_results):\n",
        "    \"\"\"Display results from evaluate.\"\"\"\n",
        "\n",
        "    metric_dicts = []\n",
        "    for eval_result in eval_results:\n",
        "        metric_dict = eval_result.metric_vals_dict\n",
        "        metric_dicts.append(metric_dict)\n",
        "\n",
        "    full_df = pd.DataFrame(metric_dicts)\n",
        "\n",
        "    hit_rate = full_df[\"hit_rate\"].mean()\n",
        "    mrr = full_df[\"mrr\"].mean()\n",
        "\n",
        "    metric_df = pd.DataFrame(\n",
        "        {\"Retriever Name\": [name], \"Hit Rate\": [hit_rate], \"MRR\": [mrr]}\n",
        "    )\n",
        "\n",
        "    return metric_df"
      ],
      "metadata": {
        "id": "cHtiCNPwWaIZ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_results(\"OpenAI Embedding Retriever\", eval_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "67ww3F-uWfuj",
        "outputId": "a5e101f4-5a52-4c7c-f956-648f5956e2ad"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               Retriever Name  Hit Rate     MRR\n",
              "0  OpenAI Embedding Retriever     0.875  0.8125"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-47912c13-65ca-49ec-b25f-790b03c08e72\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Retriever Name</th>\n",
              "      <th>Hit Rate</th>\n",
              "      <th>MRR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>OpenAI Embedding Retriever</td>\n",
              "      <td>0.875</td>\n",
              "      <td>0.8125</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-47912c13-65ca-49ec-b25f-790b03c08e72')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-47912c13-65ca-49ec-b25f-790b03c08e72 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-47912c13-65ca-49ec-b25f-790b03c08e72');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display_results(\\\"OpenAI Embedding Retriever\\\", eval_results)\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"Retriever Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"OpenAI Embedding Retriever\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Hit Rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.875,\n        \"max\": 0.875,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.875\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MRR\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.8125,\n        \"max\": 0.8125,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.8125\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MRR is less than the hit rate indicates that the top-ranking results aren't always the most relevant.  Enhancing MRR could involve the use of rerankers, which refine the order of retrieved documents."
      ],
      "metadata": {
        "id": "6fBUofiEWpg8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Response Evaluation:**"
      ],
      "metadata": {
        "id": "SUaWDMaAW7_R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "FaithfulnessEvaluator: Measures if the response from a query engine matches any source nodes which is useful for measuring if the response is hallucinated.\n",
        "\n",
        "Relevancy Evaluator: Measures if the response + source nodes match the query."
      ],
      "metadata": {
        "id": "cTVKpx20XCGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the list of queries from the above created dataset\n",
        "\n",
        "queries = list(qa_dataset.queries.values())"
      ],
      "metadata": {
        "id": "kV7sy0fhW2ye"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queries"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWSQ_w59Xp4T",
        "outputId": "ed52cbf8-94c2-4c6f-fa2a-dc5e64ba1db3"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['How does deep learning differ from traditional methods of processing data, and how is it inspired by the human brain?',\n",
              " 'Provide examples of everyday products and emerging technologies that utilize deep learning technology, and explain how it enhances their functionality.',\n",
              " 'How do businesses utilize deep learning models to analyze data and make predictions in various applications? Provide examples from the context information.',\n",
              " 'In what ways are deep learning models used in different industries such as automotive, aerospace, manufacturing, electronics, and medical research? Provide specific examples of applications mentioned in the text.',\n",
              " 'How can deep learning techniques be applied in computer vision, and what are some specific applications of computer vision mentioned in the context information?',\n",
              " 'Explain how speech recognition technology utilizes deep learning models, and provide examples of tasks that can be accomplished using speech recognition in various industries.',\n",
              " 'How do virtual assistants like Amazon Alexa and automatic transcription software utilize speech recognition technology in various tasks, and what are some specific examples provided in the text?',\n",
              " 'Explain the role of natural language processing in computer systems, citing at least two different use cases mentioned in the document.']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Faithfulness Evaluator**"
      ],
      "metadata": {
        "id": "GwtmbezSYK_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gpt-3.5-turbo\n",
        "gpt35 = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
        "service_context_gpt35 = ServiceContext.from_defaults(llm=gpt35)\n",
        "\n",
        "# gpt-4\n",
        "gpt4 = OpenAI(temperature=0, model=\"gpt-4\")\n",
        "service_context_gpt4 = ServiceContext.from_defaults(llm=gpt4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_RXXszsXr_E",
        "outputId": "13d60f23-59a6-4869-9c7e-1bf289beb3fc"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-8ced365f261a>:3: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
            "  service_context_gpt35 = ServiceContext.from_defaults(llm=gpt35)\n",
            "<ipython-input-26-8ced365f261a>:7: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
            "  service_context_gpt4 = ServiceContext.from_defaults(llm=gpt4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a QueryEngine with gpt-3.5-turbo service_context to generate response for the query."
      ],
      "metadata": {
        "id": "YyQnAGKrZJHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_index = VectorStoreIndex(nodes, service_context = service_context_gpt35)\n",
        "query_engine = vector_index.as_query_engine()"
      ],
      "metadata": {
        "id": "S0DOZB1bZJ3r"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.evaluation import FaithfulnessEvaluator\n",
        "faithfulness_gpt35 = FaithfulnessEvaluator(service_context=service_context_gpt35)"
      ],
      "metadata": {
        "id": "FFomPmzmaBgJ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_query = queries[3]\n",
        "eval_query"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "tkpXOKA-aLNZ",
        "outputId": "abd0087b-1777-4d80-ec98-8f1bb9289a04"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Provide examples of specific applications of deep learning in self-driving cars, defense systems, medical image analysis, and factory operations, highlighting the importance of automation and detection capabilities in each scenario.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response_vector = query_engine.query(eval_query)"
      ],
      "metadata": {
        "id": "kfyOPl6IaSU8"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute faithfulness evaluation\n",
        "\n",
        "eval_result = faithfulness_gpt35.evaluate_response(response=response_vector)"
      ],
      "metadata": {
        "id": "LORULAxAahHK"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can check passing parameter in eval_result if it passed the evaluation.\n",
        "eval_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkmVMoFHa7in",
        "outputId": "3e505055-7088-4a65-80db-903acd5fe9ea"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EvaluationResult(query=None, contexts=['Deep\\nlearning\\nmodels\\nare\\ncomputer\\nfiles\\nthat\\ndata\\nscientists\\nhave\\ntrained\\nto\\nperform\\ntasks\\nusing\\nan\\nalgorithm\\nor\\na\\npredefined\\nset\\nof\\nsteps.\\nBusinesses\\nuse\\ndeep\\nlearning\\nmodels\\nto\\nanalyze\\ndata\\nand\\nmake\\npredictions\\nin\\nvarious\\napplications.\\nWhat\\nare\\nthe\\nuses\\nof\\ndeep\\nlearning?\\nDeep\\nlearning\\nhas\\nseveral\\nuse\\ncases\\nin\\nautomotive,\\naerospace,\\nmanufacturing,\\nelectronics,\\nmedical\\nresearch,\\nand\\nother\\nfields.\\nThese\\nare\\nsome\\nexamples\\nof\\ndeep\\nlearning:\\n●\\nSelf-driving\\ncars\\nuse\\ndeep\\nlearning\\nmodels\\nto\\nautomatically\\ndetect\\nroad\\nsigns\\nand\\npedestrians.\\n●\\nDefense\\nsystems\\nuse\\ndeep\\nlearning\\nto\\nautomatically\\nflag\\nareas\\nof\\ninterest\\nin\\nsatellite\\nimages.\\n●\\nMedical\\nimage\\nanalysis\\nuses\\ndeep\\nlearning\\nto\\nautomatically\\ndetect\\ncancer\\ncells\\nfor\\nmedical\\ndiagnosis.\\n●\\nFactories\\nuse\\ndeep\\nlearning\\napplications\\nto\\nautomatically\\ndetect\\nwhen\\npeople\\nor\\nobjects\\nare\\nwithin\\nan\\nunsafe\\ndistance\\nof\\nmachines.', 'What\\nis\\nDeep\\nLearning?\\nDeep\\nlearning\\nis\\na\\nmethod\\nin\\nartificial\\nintelligence\\n(AI)\\nthat\\nteaches\\ncomputers\\nto\\nprocess\\ndata\\nin\\na\\nway\\nthat\\nis\\ninspired\\nby\\nthe\\nhuman\\nbrain.\\nDeep\\nlearning\\nmodels\\ncan\\nrecognize\\ncomplex\\npatterns\\nin\\npictures,\\ntext,\\nsounds,\\nand\\nother\\ndata\\nto\\nproduce\\naccurate\\ninsights\\nand\\npredictions.\\nYou\\ncan\\nuse\\ndeep\\nlearning\\nmethods\\nto\\nautomate\\ntasks\\nthat\\ntypically\\nrequire\\nhuman\\nintelligence,\\nsuch\\nas\\ndescribing\\nimages\\nor\\ntranscribing\\na\\nsound\\nfile\\ninto\\ntext.\\nWhy\\nis\\ndeep\\nlearning\\nimportant?\\nArtificial\\nintelligence\\n(AI)\\nattempts\\nto\\ntrain\\ncomputers\\nto\\nthink\\nand\\nlearn\\nas\\nhumans\\ndo.\\nDeep\\nlearning\\ntechnology\\ndrives\\nmany\\nAI\\napplications\\nused\\nin\\neveryday\\nproducts,\\nsuch\\nas\\nthe\\nfollowing:\\n●\\nDigital\\nassistants\\n●\\nVoice-activated\\ntelevision\\nremotes\\n●\\nFraud\\ndetection\\n●\\nAutomatic\\nfacial\\nrecognition\\nIt\\nis\\nalso\\na\\ncritical\\ncomponent\\nof\\nemerging\\ntechnologies\\nsuch\\nas\\nself-driving\\ncars,\\nvirtual\\nreality ,\\nand\\nmore.\\nDeep\\nlearning\\nmodels\\nare\\ncomputer\\nfiles\\nthat\\ndata\\nscientists\\nhave\\ntrained\\nto\\nperform\\ntasks\\nusing\\nan\\nalgorithm\\nor\\na\\npredefined\\nset\\nof\\nsteps.\\nBusinesses\\nuse\\ndeep\\nlearning\\nmodels\\nto\\nanalyze\\ndata\\nand\\nmake\\npredictions\\nin\\nvarious\\napplications.\\nWhat\\nare\\nthe\\nuses\\nof\\ndeep\\nlearning?\\nDeep\\nlearning\\nhas\\nseveral\\nuse\\ncases\\nin\\nautomotive,\\naerospace,\\nmanufacturing,\\nelectronics,\\nmedical\\nresearch,\\nand\\nother\\nfields.\\nThese\\nare\\nsome\\nexamples\\nof\\ndeep\\nlearning:\\n●\\nSelf-driving\\ncars\\nuse\\ndeep\\nlearning\\nmodels\\nto\\nautomatically\\ndetect\\nroad\\nsigns\\nand\\npedestrians.'], response='Self-driving cars utilize deep learning models to automatically detect road signs and pedestrians, enhancing safety and navigation. Defense systems leverage deep learning for automatic flagging of areas of interest in satellite images, aiding in surveillance and security measures. In medical image analysis, deep learning is employed to automatically detect cancer cells, facilitating early and accurate medical diagnosis. Factories utilize deep learning applications to automatically detect when people or objects are within an unsafe distance of machines, ensuring worker safety and operational efficiency. The automation and detection capabilities of deep learning in these scenarios significantly enhance performance, accuracy, and efficiency in various critical applications.', passing=True, feedback='YES', score=1.0, pairwise_source=None, invalid_result=False, invalid_reason=None)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You can check passing parameter in eval_result if it passed the evaluation.\n",
        "eval_result.passing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8UZFJYKa4wH",
        "outputId": "4ea387d2-b6a0-4b94-c909-eccafc8af237"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_result.response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "9tpBSEobbUqO",
        "outputId": "4d5f0454-7962-42a9-a031-0696260b443b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Self-driving cars utilize deep learning models to automatically detect road signs and pedestrians, enhancing safety and navigation. Defense systems leverage deep learning for automatic flagging of areas of interest in satellite images, aiding in surveillance and security measures. In medical image analysis, deep learning is employed to automatically detect cancer cells, facilitating early and accurate medical diagnosis. Factories utilize deep learning applications to automatically detect when people or objects are within an unsafe distance of machines, ensuring worker safety and operational efficiency. The automation and detection capabilities of deep learning in these scenarios significantly enhance performance, accuracy, and efficiency in various critical applications.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Relevancy Evaluator**"
      ],
      "metadata": {
        "id": "wOU03gkQbgPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.evaluation import RelevancyEvaluator\n",
        "\n",
        "relevancy_gpt35 = RelevancyEvaluator(service_context=service_context_gpt35)"
      ],
      "metadata": {
        "id": "OrnyhOcsbY9l"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick a query\n",
        "query = queries[3]\n",
        "\n",
        "query"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "xrzM30-Zbrji",
        "outputId": "78b48a9f-0bb6-46a7-d43c-e430f39af3af"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Provide examples of specific applications of deep learning in self-driving cars, defense systems, medical image analysis, and factory operations, highlighting the importance of automation and detection capabilities in each scenario.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate response.\n",
        "# response_vector has response and source nodes (retrieved context)\n",
        "response_vector = query_engine.query(query)\n",
        "\n",
        "# Relevancy evaluation\n",
        "eval_result = relevancy_gpt35.evaluate_response(\n",
        "    query=query, response=response_vector\n",
        ")"
      ],
      "metadata": {
        "id": "Lx794DoxbyFk"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_result.feedback"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "MzOqXD5Xb7Ea",
        "outputId": "61c23853-3d72-43fa-bbeb-c5a09fe92b81"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'YES'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_result.passing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5XYp__3cB9n",
        "outputId": "b9a41f95-fe2b-4e2c-85a6-83f9f4e3a6c5"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.evaluation import BatchEvalRunner\n",
        "\n",
        "# Let's pick top 10 queries to do evaluation\n",
        "batch_eval_queries = queries[:2]\n",
        "\n",
        "# Initiate BatchEvalRunner to compute FaithFulness and Relevancy Evaluation.\n",
        "runner = BatchEvalRunner(\n",
        "    {\"faithfulness\": faithfulness_gpt35, \"relevancy\": relevancy_gpt35},\n",
        "    workers=8,\n",
        ")\n",
        "\n",
        "# Compute evaluation\n",
        "eval_results = await runner.aevaluate_queries(\n",
        "    query_engine, queries=batch_eval_queries\n",
        ")"
      ],
      "metadata": {
        "id": "vP4ofW3zcPEL"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's get faithfulness score\n",
        "\n",
        "faithfulness_score = sum(result.passing for result in eval_results['faithfulness']) / len(eval_results['faithfulness'])\n",
        "\n",
        "faithfulness_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9MgW8H8dIiQ",
        "outputId": "e40d1409-a51b-44de-f6d3-2cfd275909eb"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's get relevancy score\n",
        "\n",
        "relevancy_score = sum(result.passing for result in eval_results['relevancy']) / len(eval_results['relevancy'])\n",
        "\n",
        "relevancy_score\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8M4eOqydjsZ",
        "outputId": "5fece739-d50d-4d5c-c543-778fe03c36e9"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Faithfulness score of 1.0 signifies that the generated answers contain no hallucinations and are entirely based on retrieved context.\n",
        "\n",
        "Relevancy score of 1.0 suggests that the answers generated are consistently aligned with the retrieved context and the queries."
      ],
      "metadata": {
        "id": "zPVCCMOZdrNH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "quaRYfKadqXy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}